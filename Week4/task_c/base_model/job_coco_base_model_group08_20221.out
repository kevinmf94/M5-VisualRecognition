[32m[03/28 19:21:13 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): Res5ROIHeads(
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=2048, out_features=3, bias=True)
      (bbox_pred): Linear(in_features=2048, out_features=8, bias=True)
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (deconv): ConvTranspose2d(2048, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[03/28 19:21:13 d2.data.build]: [0mRemoved 729 images with no usable annotations. 3790 images left.
[32m[03/28 19:21:14 d2.data.build]: [0mDistribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|    Cars    | 16900        | Pedestrian | 690          |
|            |              |            |              |
|   total    | 17590        |            |              |[0m
[32m[03/28 19:21:14 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[03/28 19:21:14 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/28 19:21:14 d2.data.common]: [0mSerializing 3790 elements to byte tensors and concatenating them all ...
[32m[03/28 19:21:14 d2.data.common]: [0mSerialized dataset takes 5.51 MiB
[32m[03/28 19:21:14 d2.data.build]: [0mRemoved 117 images with no usable annotations. 3372 images left.
[32m[03/28 19:21:14 d2.data.build]: [0mDistribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|    Cars    | 9960         | Pedestrian | 10725        |
|            |              |            |              |
|   total    | 20685        |            |              |[0m
[32m[03/28 19:21:14 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[03/28 19:21:14 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/28 19:21:14 d2.data.common]: [0mSerializing 3372 elements to byte tensors and concatenating them all ...
[32m[03/28 19:21:14 d2.data.common]: [0mSerialized dataset takes 6.00 MiB
[32m[03/28 19:21:15 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[03/28 19:21:39 d2.utils.events]: [0m eta: 0:02:29  iter: 19  total_loss: 1.721  loss_cls: 0.6593  loss_box_reg: 0.3032  loss_mask: 0.6572  loss_rpn_cls: 0.01725  loss_rpn_loc: 0.006796  total_val_loss: 1.921  val_loss_cls: 0.6427  val_loss_box_reg: 0.4608  val_loss_mask: 0.6695  val_loss_rpn_cls: 0.03318  val_loss_rpn_loc: 0.01166  time: 0.8350  data_time: 0.0237  lr: 0.00019981  max_mem: 4742M
[32m[03/28 19:22:03 d2.utils.events]: [0m eta: 0:02:13  iter: 39  total_loss: 0.96  loss_cls: 0.1895  loss_box_reg: 0.4058  loss_mask: 0.3718  loss_rpn_cls: 0.01519  loss_rpn_loc: 0.008274  total_val_loss: 1.465  val_loss_cls: 0.3047  val_loss_box_reg: 0.5132  val_loss_mask: 0.5003  val_loss_rpn_cls: 0.03126  val_loss_rpn_loc: 0.01274  time: 0.8378  data_time: 0.0062  lr: 0.00039961  max_mem: 4742M
[32m[03/28 19:22:26 d2.utils.events]: [0m eta: 0:01:57  iter: 59  total_loss: 0.7524  loss_cls: 0.1003  loss_box_reg: 0.4141  loss_mask: 0.1928  loss_rpn_cls: 0.0141  loss_rpn_loc: 0.01441  total_val_loss: 1.084  val_loss_cls: 0.1984  val_loss_box_reg: 0.3424  val_loss_mask: 0.4132  val_loss_rpn_cls: 0.0304  val_loss_rpn_loc: 0.01151  time: 0.8410  data_time: 0.0060  lr: 0.00059941  max_mem: 4742M
[32m[03/28 19:22:50 d2.utils.events]: [0m eta: 0:01:40  iter: 79  total_loss: 0.4188  loss_cls: 0.05042  loss_box_reg: 0.173  loss_mask: 0.1458  loss_rpn_cls: 0.01196  loss_rpn_loc: 0.005383  total_val_loss: 1.128  val_loss_cls: 0.2281  val_loss_box_reg: 0.4257  val_loss_mask: 0.3534  val_loss_rpn_cls: 0.02534  val_loss_rpn_loc: 0.01497  time: 0.8436  data_time: 0.0084  lr: 0.00079921  max_mem: 4742M
[32m[03/28 19:23:14 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/28 19:23:14 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/28 19:23:14 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/28 19:23:14 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/28 19:23:15 d2.utils.events]: [0m eta: 0:01:24  iter: 99  total_loss: 0.4928  loss_cls: 0.07635  loss_box_reg: 0.169  loss_mask: 0.1895  loss_rpn_cls: 0.0082  loss_rpn_loc: 0.0101  total_val_loss: 0.8906  val_loss_cls: 0.1881  val_loss_box_reg: 0.3514  val_loss_mask: 0.3018  val_loss_rpn_cls: 0.0169  val_loss_rpn_loc: 0.01646  time: 0.8465  data_time: 0.0074  lr: 0.00099901  max_mem: 4744M
[32m[03/28 19:23:39 d2.utils.events]: [0m eta: 0:01:07  iter: 119  total_loss: 0.4197  loss_cls: 0.06468  loss_box_reg: 0.1423  loss_mask: 0.1714  loss_rpn_cls: 0.008159  loss_rpn_loc: 0.01229  total_val_loss: 0.7696  val_loss_cls: 0.1654  val_loss_box_reg: 0.251  val_loss_mask: 0.3322  val_loss_rpn_cls: 0.01503  val_loss_rpn_loc: 0.01116  time: 0.8507  data_time: 0.0075  lr: 0.0011988  max_mem: 4744M
[32m[03/28 19:24:03 d2.utils.events]: [0m eta: 0:00:50  iter: 139  total_loss: 0.3684  loss_cls: 0.05136  loss_box_reg: 0.1037  loss_mask: 0.1471  loss_rpn_cls: 0.0109  loss_rpn_loc: 0.009053  total_val_loss: 0.9991  val_loss_cls: 0.1844  val_loss_box_reg: 0.2652  val_loss_mask: 0.4177  val_loss_rpn_cls: 0.02359  val_loss_rpn_loc: 0.01415  time: 0.8526  data_time: 0.0061  lr: 0.0013986  max_mem: 4744M
[32m[03/28 19:24:28 d2.utils.events]: [0m eta: 0:00:34  iter: 159  total_loss: 0.3482  loss_cls: 0.05172  loss_box_reg: 0.1302  loss_mask: 0.1602  loss_rpn_cls: 0.004522  loss_rpn_loc: 0.009469  total_val_loss: 0.6007  val_loss_cls: 0.1156  val_loss_box_reg: 0.1883  val_loss_mask: 0.2341  val_loss_rpn_cls: 0.01575  val_loss_rpn_loc: 0.01247  time: 0.8553  data_time: 0.0069  lr: 0.0015984  max_mem: 4744M
[32m[03/28 19:24:52 d2.utils.events]: [0m eta: 0:00:17  iter: 179  total_loss: 0.3861  loss_cls: 0.06685  loss_box_reg: 0.1512  loss_mask: 0.1602  loss_rpn_cls: 0.007975  loss_rpn_loc: 0.01158  total_val_loss: 0.7434  val_loss_cls: 0.1875  val_loss_box_reg: 0.3048  val_loss_mask: 0.3828  val_loss_rpn_cls: 0.01748  val_loss_rpn_loc: 0.01577  time: 0.8573  data_time: 0.0068  lr: 0.0017982  max_mem: 4744M
[32m[03/28 19:25:17 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/28 19:25:17 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/28 19:25:17 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/28 19:25:17 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/28 19:25:18 d2.utils.events]: [0m eta: 0:00:00  iter: 199  total_loss: 0.3646  loss_cls: 0.04391  loss_box_reg: 0.1025  loss_mask: 0.2017  loss_rpn_cls: 0.008815  loss_rpn_loc: 0.00788  total_val_loss: 0.7148  val_loss_cls: 0.1077  val_loss_box_reg: 0.2101  val_loss_mask: 0.2921  val_loss_rpn_cls: 0.0106  val_loss_rpn_loc: 0.01248  time: 0.8582  data_time: 0.0067  lr: 0.001998  max_mem: 4744M
[32m[03/28 19:25:18 d2.engine.hooks]: [0mOverall training speed: 198 iterations in 0:02:49 (0.8583 s / it)
[32m[03/28 19:25:18 d2.engine.hooks]: [0mTotal training time: 0:04:00 (0:01:10 on hooks)
[32m[03/28 19:25:18 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/28 19:25:18 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/28 19:25:18 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/28 19:25:18 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[5m[31mWARNING[0m [32m[03/28 19:25:18 d2.evaluation.coco_evaluation]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass tasks in directly
[32m[03/28 19:25:18 d2.evaluation.coco_evaluation]: [0m'kittimots_val' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...
[32m[03/28 19:25:18 d2.data.datasets.coco]: [0mConverting annotations of dataset 'kittimots_val' to COCO format ...)
[32m[03/28 19:25:18 d2.data.datasets.coco]: [0mConverting dataset dicts into COCO format
[32m[03/28 19:25:18 d2.data.datasets.coco]: [0mConversion finished, #images: 3489, #annotations: 20685
[32m[03/28 19:25:18 d2.data.datasets.coco]: [0mCaching COCO format annotations at './output/kittimots_val_coco_format.json' ...
[32m[03/28 19:25:19 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/28 19:25:19 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/28 19:25:19 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[32m[03/28 19:25:20 d2.evaluation.evaluator]: [0mStart inference on 3489 images
[32m[03/28 19:25:23 d2.evaluation.evaluator]: [0mInference done 11/3489. 0.2263 s / img. ETA=0:14:08
[32m[03/28 19:25:28 d2.evaluation.evaluator]: [0mInference done 32/3489. 0.2294 s / img. ETA=0:14:10
[32m[03/28 19:25:33 d2.evaluation.evaluator]: [0mInference done 53/3489. 0.2297 s / img. ETA=0:14:03
[32m[03/28 19:25:38 d2.evaluation.evaluator]: [0mInference done 74/3489. 0.2288 s / img. ETA=0:13:51
[32m[03/28 19:25:43 d2.evaluation.evaluator]: [0mInference done 94/3489. 0.2299 s / img. ETA=0:13:57
[32m[03/28 19:25:48 d2.evaluation.evaluator]: [0mInference done 112/3489. 0.2319 s / img. ETA=0:14:12
[32m[03/28 19:25:53 d2.evaluation.evaluator]: [0mInference done 128/3489. 0.2343 s / img. ETA=0:14:42
[32m[03/28 19:25:58 d2.evaluation.evaluator]: [0mInference done 142/3489. 0.2368 s / img. ETA=0:15:11
[32m[03/28 19:26:03 d2.evaluation.evaluator]: [0mInference done 156/3489. 0.2385 s / img. ETA=0:15:34
[32m[03/28 19:26:08 d2.evaluation.evaluator]: [0mInference done 170/3489. 0.2400 s / img. ETA=0:15:52
[32m[03/28 19:26:14 d2.evaluation.evaluator]: [0mInference done 186/3489. 0.2408 s / img. ETA=0:15:58
[32m[03/28 19:26:19 d2.evaluation.evaluator]: [0mInference done 204/3489. 0.2408 s / img. ETA=0:15:51
[32m[03/28 19:26:24 d2.evaluation.evaluator]: [0mInference done 224/3489. 0.2400 s / img. ETA=0:15:35
[32m[03/28 19:26:29 d2.evaluation.evaluator]: [0mInference done 244/3489. 0.2396 s / img. ETA=0:15:23
[32m[03/28 19:26:34 d2.evaluation.evaluator]: [0mInference done 263/3489. 0.2395 s / img. ETA=0:15:15
[32m[03/28 19:26:39 d2.evaluation.evaluator]: [0mInference done 281/3489. 0.2396 s / img. ETA=0:15:10
[32m[03/28 19:26:45 d2.evaluation.evaluator]: [0mInference done 300/3489. 0.2394 s / img. ETA=0:15:03
[32m[03/28 19:26:50 d2.evaluation.evaluator]: [0mInference done 317/3489. 0.2397 s / img. ETA=0:15:00
[32m[03/28 19:26:55 d2.evaluation.evaluator]: [0mInference done 335/3489. 0.2399 s / img. ETA=0:14:55
[32m[03/28 19:27:00 d2.evaluation.evaluator]: [0mInference done 351/3489. 0.2403 s / img. ETA=0:14:56
[32m[03/28 19:27:05 d2.evaluation.evaluator]: [0mInference done 368/3489. 0.2405 s / img. ETA=0:14:53
[32m[03/28 19:27:10 d2.evaluation.evaluator]: [0mInference done 386/3489. 0.2405 s / img. ETA=0:14:48
[32m[03/28 19:27:15 d2.evaluation.evaluator]: [0mInference done 405/3489. 0.2405 s / img. ETA=0:14:40
[32m[03/28 19:27:20 d2.evaluation.evaluator]: [0mInference done 423/3489. 0.2405 s / img. ETA=0:14:34
[32m[03/28 19:27:26 d2.evaluation.evaluator]: [0mInference done 444/3489. 0.2400 s / img. ETA=0:14:22
[32m[03/28 19:27:31 d2.evaluation.evaluator]: [0mInference done 465/3489. 0.2396 s / img. ETA=0:14:11
[32m[03/28 19:27:36 d2.evaluation.evaluator]: [0mInference done 486/3489. 0.2391 s / img. ETA=0:14:00
[32m[03/28 19:27:41 d2.evaluation.evaluator]: [0mInference done 507/3489. 0.2388 s / img. ETA=0:13:49
[32m[03/28 19:27:46 d2.evaluation.evaluator]: [0mInference done 528/3489. 0.2384 s / img. ETA=0:13:39
[32m[03/28 19:27:51 d2.evaluation.evaluator]: [0mInference done 549/3489. 0.2381 s / img. ETA=0:13:29
[32m[03/28 19:27:56 d2.evaluation.evaluator]: [0mInference done 568/3489. 0.2381 s / img. ETA=0:13:23
[32m[03/28 19:28:01 d2.evaluation.evaluator]: [0mInference done 586/3489. 0.2382 s / img. ETA=0:13:19
[32m[03/28 19:28:06 d2.evaluation.evaluator]: [0mInference done 605/3489. 0.2383 s / img. ETA=0:13:13
[32m[03/28 19:28:12 d2.evaluation.evaluator]: [0mInference done 624/3489. 0.2383 s / img. ETA=0:13:08
[32m[03/28 19:28:17 d2.evaluation.evaluator]: [0mInference done 643/3489. 0.2384 s / img. ETA=0:13:03
[32m[03/28 19:28:22 d2.evaluation.evaluator]: [0mInference done 662/3489. 0.2384 s / img. ETA=0:12:57
[32m[03/28 19:28:27 d2.evaluation.evaluator]: [0mInference done 682/3489. 0.2382 s / img. ETA=0:12:50
[32m[03/28 19:28:32 d2.evaluation.evaluator]: [0mInference done 703/3489. 0.2380 s / img. ETA=0:12:42
[32m[03/28 19:28:37 d2.evaluation.evaluator]: [0mInference done 723/3489. 0.2378 s / img. ETA=0:12:35
[32m[03/28 19:28:42 d2.evaluation.evaluator]: [0mInference done 743/3489. 0.2377 s / img. ETA=0:12:28
[32m[03/28 19:28:47 d2.evaluation.evaluator]: [0mInference done 764/3489. 0.2375 s / img. ETA=0:12:20
[32m[03/28 19:28:52 d2.evaluation.evaluator]: [0mInference done 785/3489. 0.2373 s / img. ETA=0:12:12
[32m[03/28 19:28:57 d2.evaluation.evaluator]: [0mInference done 806/3489. 0.2372 s / img. ETA=0:12:04
[32m[03/28 19:29:03 d2.evaluation.evaluator]: [0mInference done 827/3489. 0.2370 s / img. ETA=0:11:57
[32m[03/28 19:29:08 d2.evaluation.evaluator]: [0mInference done 848/3489. 0.2369 s / img. ETA=0:11:49
[32m[03/28 19:29:13 d2.evaluation.evaluator]: [0mInference done 869/3489. 0.2367 s / img. ETA=0:11:42
[32m[03/28 19:29:18 d2.evaluation.evaluator]: [0mInference done 891/3489. 0.2364 s / img. ETA=0:11:34
[32m[03/28 19:29:23 d2.evaluation.evaluator]: [0mInference done 909/3489. 0.2365 s / img. ETA=0:11:30
[32m[03/28 19:29:28 d2.evaluation.evaluator]: [0mInference done 925/3489. 0.2367 s / img. ETA=0:11:28
[32m[03/28 19:29:33 d2.evaluation.evaluator]: [0mInference done 940/3489. 0.2370 s / img. ETA=0:11:27
[32m[03/28 19:29:39 d2.evaluation.evaluator]: [0mInference done 956/3489. 0.2372 s / img. ETA=0:11:25
[32m[03/28 19:29:44 d2.evaluation.evaluator]: [0mInference done 971/3489. 0.2374 s / img. ETA=0:11:24
[32m[03/28 19:29:49 d2.evaluation.evaluator]: [0mInference done 986/3489. 0.2376 s / img. ETA=0:11:23
[32m[03/28 19:29:54 d2.evaluation.evaluator]: [0mInference done 1001/3489. 0.2378 s / img. ETA=0:11:21
[32m[03/28 19:29:59 d2.evaluation.evaluator]: [0mInference done 1016/3489. 0.2380 s / img. ETA=0:11:20
[32m[03/28 19:30:04 d2.evaluation.evaluator]: [0mInference done 1031/3489. 0.2383 s / img. ETA=0:11:18
[32m[03/28 19:30:10 d2.evaluation.evaluator]: [0mInference done 1046/3489. 0.2384 s / img. ETA=0:11:17
[32m[03/28 19:30:15 d2.evaluation.evaluator]: [0mInference done 1061/3489. 0.2386 s / img. ETA=0:11:14
[32m[03/28 19:30:20 d2.evaluation.evaluator]: [0mInference done 1077/3489. 0.2387 s / img. ETA=0:11:12
[32m[03/28 19:30:25 d2.evaluation.evaluator]: [0mInference done 1095/3489. 0.2388 s / img. ETA=0:11:07
[32m[03/28 19:30:30 d2.evaluation.evaluator]: [0mInference done 1114/3489. 0.2389 s / img. ETA=0:11:01
[32m[03/28 19:30:35 d2.evaluation.evaluator]: [0mInference done 1131/3489. 0.2390 s / img. ETA=0:10:57
[32m[03/28 19:30:40 d2.evaluation.evaluator]: [0mInference done 1147/3489. 0.2391 s / img. ETA=0:10:54
[32m[03/28 19:30:46 d2.evaluation.evaluator]: [0mInference done 1163/3489. 0.2392 s / img. ETA=0:10:51
[32m[03/28 19:30:51 d2.evaluation.evaluator]: [0mInference done 1179/3489. 0.2393 s / img. ETA=0:10:48
[32m[03/28 19:30:56 d2.evaluation.evaluator]: [0mInference done 1197/3489. 0.2393 s / img. ETA=0:10:43
[32m[03/28 19:31:01 d2.evaluation.evaluator]: [0mInference done 1215/3489. 0.2393 s / img. ETA=0:10:38
[32m[03/28 19:31:06 d2.evaluation.evaluator]: [0mInference done 1234/3489. 0.2393 s / img. ETA=0:10:32
[32m[03/28 19:31:11 d2.evaluation.evaluator]: [0mInference done 1252/3489. 0.2393 s / img. ETA=0:10:27
[32m[03/28 19:31:16 d2.evaluation.evaluator]: [0mInference done 1274/3489. 0.2391 s / img. ETA=0:10:19
[32m[03/28 19:31:21 d2.evaluation.evaluator]: [0mInference done 1296/3489. 0.2389 s / img. ETA=0:10:11
[32m[03/28 19:31:27 d2.evaluation.evaluator]: [0mInference done 1318/3489. 0.2387 s / img. ETA=0:10:04
[32m[03/28 19:31:32 d2.evaluation.evaluator]: [0mInference done 1339/3489. 0.2385 s / img. ETA=0:09:57
[32m[03/28 19:31:37 d2.evaluation.evaluator]: [0mInference done 1360/3489. 0.2384 s / img. ETA=0:09:50
[32m[03/28 19:31:42 d2.evaluation.evaluator]: [0mInference done 1381/3489. 0.2382 s / img. ETA=0:09:43
[32m[03/28 19:31:47 d2.evaluation.evaluator]: [0mInference done 1402/3489. 0.2381 s / img. ETA=0:09:36
[32m[03/28 19:31:52 d2.evaluation.evaluator]: [0mInference done 1423/3489. 0.2380 s / img. ETA=0:09:29
[32m[03/28 19:31:57 d2.evaluation.evaluator]: [0mInference done 1444/3489. 0.2379 s / img. ETA=0:09:22
[32m[03/28 19:32:02 d2.evaluation.evaluator]: [0mInference done 1465/3489. 0.2378 s / img. ETA=0:09:15
[32m[03/28 19:32:07 d2.evaluation.evaluator]: [0mInference done 1486/3489. 0.2376 s / img. ETA=0:09:09
[32m[03/28 19:32:12 d2.evaluation.evaluator]: [0mInference done 1508/3489. 0.2375 s / img. ETA=0:09:01
[32m[03/28 19:32:17 d2.evaluation.evaluator]: [0mInference done 1529/3489. 0.2374 s / img. ETA=0:08:55
[32m[03/28 19:32:23 d2.evaluation.evaluator]: [0mInference done 1550/3489. 0.2373 s / img. ETA=0:08:48
[32m[03/28 19:32:28 d2.evaluation.evaluator]: [0mInference done 1571/3489. 0.2372 s / img. ETA=0:08:42
[32m[03/28 19:32:33 d2.evaluation.evaluator]: [0mInference done 1592/3489. 0.2371 s / img. ETA=0:08:35
[32m[03/28 19:32:38 d2.evaluation.evaluator]: [0mInference done 1611/3489. 0.2371 s / img. ETA=0:08:30
[32m[03/28 19:32:43 d2.evaluation.evaluator]: [0mInference done 1629/3489. 0.2372 s / img. ETA=0:08:26
[32m[03/28 19:32:48 d2.evaluation.evaluator]: [0mInference done 1648/3489. 0.2372 s / img. ETA=0:08:20
[32m[03/28 19:32:53 d2.evaluation.evaluator]: [0mInference done 1666/3489. 0.2372 s / img. ETA=0:08:16
[32m[03/28 19:32:58 d2.evaluation.evaluator]: [0mInference done 1683/3489. 0.2373 s / img. ETA=0:08:12
[32m[03/28 19:33:04 d2.evaluation.evaluator]: [0mInference done 1699/3489. 0.2374 s / img. ETA=0:08:08
[32m[03/28 19:33:09 d2.evaluation.evaluator]: [0mInference done 1716/3489. 0.2375 s / img. ETA=0:08:04
[32m[03/28 19:33:14 d2.evaluation.evaluator]: [0mInference done 1732/3489. 0.2377 s / img. ETA=0:08:01
[32m[03/28 19:33:19 d2.evaluation.evaluator]: [0mInference done 1748/3489. 0.2378 s / img. ETA=0:07:57
[32m[03/28 19:33:25 d2.evaluation.evaluator]: [0mInference done 1763/3489. 0.2379 s / img. ETA=0:07:54
[32m[03/28 19:33:30 d2.evaluation.evaluator]: [0mInference done 1778/3489. 0.2380 s / img. ETA=0:07:51
[32m[03/28 19:33:35 d2.evaluation.evaluator]: [0mInference done 1793/3489. 0.2381 s / img. ETA=0:07:48
[32m[03/28 19:33:40 d2.evaluation.evaluator]: [0mInference done 1808/3489. 0.2382 s / img. ETA=0:07:45
[32m[03/28 19:33:46 d2.evaluation.evaluator]: [0mInference done 1823/3489. 0.2383 s / img. ETA=0:07:42
[32m[03/28 19:33:51 d2.evaluation.evaluator]: [0mInference done 1838/3489. 0.2384 s / img. ETA=0:07:39
[32m[03/28 19:33:56 d2.evaluation.evaluator]: [0mInference done 1853/3489. 0.2385 s / img. ETA=0:07:35
[32m[03/28 19:34:01 d2.evaluation.evaluator]: [0mInference done 1868/3489. 0.2386 s / img. ETA=0:07:32
[32m[03/28 19:34:06 d2.evaluation.evaluator]: [0mInference done 1883/3489. 0.2387 s / img. ETA=0:07:29
[32m[03/28 19:34:12 d2.evaluation.evaluator]: [0mInference done 1897/3489. 0.2388 s / img. ETA=0:07:26
[32m[03/28 19:34:17 d2.evaluation.evaluator]: [0mInference done 1912/3489. 0.2389 s / img. ETA=0:07:23
[32m[03/28 19:34:22 d2.evaluation.evaluator]: [0mInference done 1927/3489. 0.2390 s / img. ETA=0:07:19
[32m[03/28 19:34:27 d2.evaluation.evaluator]: [0mInference done 1942/3489. 0.2391 s / img. ETA=0:07:16
[32m[03/28 19:34:32 d2.evaluation.evaluator]: [0mInference done 1957/3489. 0.2392 s / img. ETA=0:07:12
[32m[03/28 19:34:38 d2.evaluation.evaluator]: [0mInference done 1972/3489. 0.2393 s / img. ETA=0:07:09
[32m[03/28 19:34:43 d2.evaluation.evaluator]: [0mInference done 1987/3489. 0.2393 s / img. ETA=0:07:05
[32m[03/28 19:34:48 d2.evaluation.evaluator]: [0mInference done 2004/3489. 0.2394 s / img. ETA=0:07:01
[32m[03/28 19:34:53 d2.evaluation.evaluator]: [0mInference done 2022/3489. 0.2395 s / img. ETA=0:06:56
[32m[03/28 19:34:58 d2.evaluation.evaluator]: [0mInference done 2039/3489. 0.2395 s / img. ETA=0:06:51
[32m[03/28 19:35:04 d2.evaluation.evaluator]: [0mInference done 2055/3489. 0.2395 s / img. ETA=0:06:47
[32m[03/28 19:35:09 d2.evaluation.evaluator]: [0mInference done 2071/3489. 0.2396 s / img. ETA=0:06:43
[32m[03/28 19:35:14 d2.evaluation.evaluator]: [0mInference done 2086/3489. 0.2397 s / img. ETA=0:06:39
[32m[03/28 19:35:19 d2.evaluation.evaluator]: [0mInference done 2101/3489. 0.2397 s / img. ETA=0:06:36
[32m[03/28 19:35:24 d2.evaluation.evaluator]: [0mInference done 2116/3489. 0.2398 s / img. ETA=0:06:32
[32m[03/28 19:35:29 d2.evaluation.evaluator]: [0mInference done 2131/3489. 0.2399 s / img. ETA=0:06:28
[32m[03/28 19:35:35 d2.evaluation.evaluator]: [0mInference done 2146/3489. 0.2400 s / img. ETA=0:06:24
[32m[03/28 19:35:40 d2.evaluation.evaluator]: [0mInference done 2161/3489. 0.2401 s / img. ETA=0:06:21
[32m[03/28 19:35:45 d2.evaluation.evaluator]: [0mInference done 2176/3489. 0.2402 s / img. ETA=0:06:17
[32m[03/28 19:35:50 d2.evaluation.evaluator]: [0mInference done 2191/3489. 0.2403 s / img. ETA=0:06:13
[32m[03/28 19:35:56 d2.evaluation.evaluator]: [0mInference done 2206/3489. 0.2403 s / img. ETA=0:06:09
[32m[03/28 19:36:01 d2.evaluation.evaluator]: [0mInference done 2221/3489. 0.2404 s / img. ETA=0:06:05
[32m[03/28 19:36:06 d2.evaluation.evaluator]: [0mInference done 2236/3489. 0.2405 s / img. ETA=0:06:02
[32m[03/28 19:36:11 d2.evaluation.evaluator]: [0mInference done 2251/3489. 0.2405 s / img. ETA=0:05:58
[32m[03/28 19:36:16 d2.evaluation.evaluator]: [0mInference done 2268/3489. 0.2406 s / img. ETA=0:05:53
[32m[03/28 19:36:21 d2.evaluation.evaluator]: [0mInference done 2284/3489. 0.2406 s / img. ETA=0:05:49
[32m[03/28 19:36:26 d2.evaluation.evaluator]: [0mInference done 2299/3489. 0.2406 s / img. ETA=0:05:45
[32m[03/28 19:36:31 d2.evaluation.evaluator]: [0mInference done 2313/3489. 0.2407 s / img. ETA=0:05:41
[32m[03/28 19:36:37 d2.evaluation.evaluator]: [0mInference done 2328/3489. 0.2408 s / img. ETA=0:05:37
[32m[03/28 19:36:42 d2.evaluation.evaluator]: [0mInference done 2342/3489. 0.2408 s / img. ETA=0:05:34
[32m[03/28 19:36:47 d2.evaluation.evaluator]: [0mInference done 2357/3489. 0.2408 s / img. ETA=0:05:30
[32m[03/28 19:36:52 d2.evaluation.evaluator]: [0mInference done 2375/3489. 0.2408 s / img. ETA=0:05:24
[32m[03/28 19:36:57 d2.evaluation.evaluator]: [0mInference done 2394/3489. 0.2408 s / img. ETA=0:05:19
[32m[03/28 19:37:03 d2.evaluation.evaluator]: [0mInference done 2412/3489. 0.2408 s / img. ETA=0:05:13
[32m[03/28 19:37:08 d2.evaluation.evaluator]: [0mInference done 2430/3489. 0.2408 s / img. ETA=0:05:08
[32m[03/28 19:37:13 d2.evaluation.evaluator]: [0mInference done 2446/3489. 0.2409 s / img. ETA=0:05:03
[32m[03/28 19:37:18 d2.evaluation.evaluator]: [0mInference done 2462/3489. 0.2409 s / img. ETA=0:04:59
[32m[03/28 19:37:23 d2.evaluation.evaluator]: [0mInference done 2477/3489. 0.2410 s / img. ETA=0:04:55
[32m[03/28 19:37:28 d2.evaluation.evaluator]: [0mInference done 2494/3489. 0.2410 s / img. ETA=0:04:50
[32m[03/28 19:37:33 d2.evaluation.evaluator]: [0mInference done 2515/3489. 0.2409 s / img. ETA=0:04:44
[32m[03/28 19:37:38 d2.evaluation.evaluator]: [0mInference done 2535/3489. 0.2408 s / img. ETA=0:04:37
[32m[03/28 19:37:43 d2.evaluation.evaluator]: [0mInference done 2555/3489. 0.2408 s / img. ETA=0:04:31
[32m[03/28 19:37:49 d2.evaluation.evaluator]: [0mInference done 2575/3489. 0.2407 s / img. ETA=0:04:25
[32m[03/28 19:37:54 d2.evaluation.evaluator]: [0mInference done 2596/3489. 0.2406 s / img. ETA=0:04:19
[32m[03/28 19:37:59 d2.evaluation.evaluator]: [0mInference done 2617/3489. 0.2406 s / img. ETA=0:04:12
[32m[03/28 19:38:04 d2.evaluation.evaluator]: [0mInference done 2638/3489. 0.2405 s / img. ETA=0:04:06
[32m[03/28 19:38:09 d2.evaluation.evaluator]: [0mInference done 2659/3489. 0.2404 s / img. ETA=0:04:00
[32m[03/28 19:38:14 d2.evaluation.evaluator]: [0mInference done 2679/3489. 0.2403 s / img. ETA=0:03:54
[32m[03/28 19:38:19 d2.evaluation.evaluator]: [0mInference done 2699/3489. 0.2403 s / img. ETA=0:03:48
[32m[03/28 19:38:24 d2.evaluation.evaluator]: [0mInference done 2719/3489. 0.2402 s / img. ETA=0:03:42
[32m[03/28 19:38:29 d2.evaluation.evaluator]: [0mInference done 2739/3489. 0.2402 s / img. ETA=0:03:36
[32m[03/28 19:38:35 d2.evaluation.evaluator]: [0mInference done 2759/3489. 0.2401 s / img. ETA=0:03:30
[32m[03/28 19:38:40 d2.evaluation.evaluator]: [0mInference done 2779/3489. 0.2400 s / img. ETA=0:03:24
[32m[03/28 19:38:45 d2.evaluation.evaluator]: [0mInference done 2799/3489. 0.2400 s / img. ETA=0:03:18
[32m[03/28 19:38:50 d2.evaluation.evaluator]: [0mInference done 2819/3489. 0.2399 s / img. ETA=0:03:12
[32m[03/28 19:38:55 d2.evaluation.evaluator]: [0mInference done 2839/3489. 0.2399 s / img. ETA=0:03:06
[32m[03/28 19:39:00 d2.evaluation.evaluator]: [0mInference done 2859/3489. 0.2398 s / img. ETA=0:03:00
[32m[03/28 19:39:05 d2.evaluation.evaluator]: [0mInference done 2880/3489. 0.2397 s / img. ETA=0:02:54
[32m[03/28 19:39:10 d2.evaluation.evaluator]: [0mInference done 2901/3489. 0.2397 s / img. ETA=0:02:48
[32m[03/28 19:39:16 d2.evaluation.evaluator]: [0mInference done 2922/3489. 0.2396 s / img. ETA=0:02:42
[32m[03/28 19:39:21 d2.evaluation.evaluator]: [0mInference done 2943/3489. 0.2396 s / img. ETA=0:02:36
[32m[03/28 19:39:26 d2.evaluation.evaluator]: [0mInference done 2964/3489. 0.2395 s / img. ETA=0:02:29
[32m[03/28 19:39:31 d2.evaluation.evaluator]: [0mInference done 2985/3489. 0.2394 s / img. ETA=0:02:23
[32m[03/28 19:39:36 d2.evaluation.evaluator]: [0mInference done 3005/3489. 0.2394 s / img. ETA=0:02:17
[32m[03/28 19:39:41 d2.evaluation.evaluator]: [0mInference done 3025/3489. 0.2393 s / img. ETA=0:02:12
[32m[03/28 19:39:46 d2.evaluation.evaluator]: [0mInference done 3045/3489. 0.2393 s / img. ETA=0:02:06
[32m[03/28 19:39:51 d2.evaluation.evaluator]: [0mInference done 3065/3489. 0.2392 s / img. ETA=0:02:00
[32m[03/28 19:39:56 d2.evaluation.evaluator]: [0mInference done 3085/3489. 0.2392 s / img. ETA=0:01:54
[32m[03/28 19:40:02 d2.evaluation.evaluator]: [0mInference done 3105/3489. 0.2391 s / img. ETA=0:01:49
[32m[03/28 19:40:07 d2.evaluation.evaluator]: [0mInference done 3125/3489. 0.2391 s / img. ETA=0:01:43
[32m[03/28 19:40:12 d2.evaluation.evaluator]: [0mInference done 3145/3489. 0.2391 s / img. ETA=0:01:37
[32m[03/28 19:40:17 d2.evaluation.evaluator]: [0mInference done 3165/3489. 0.2390 s / img. ETA=0:01:31
[32m[03/28 19:40:22 d2.evaluation.evaluator]: [0mInference done 3186/3489. 0.2390 s / img. ETA=0:01:25
[32m[03/28 19:40:27 d2.evaluation.evaluator]: [0mInference done 3206/3489. 0.2389 s / img. ETA=0:01:20
[32m[03/28 19:40:32 d2.evaluation.evaluator]: [0mInference done 3226/3489. 0.2389 s / img. ETA=0:01:14
[32m[03/28 19:40:38 d2.evaluation.evaluator]: [0mInference done 3246/3489. 0.2388 s / img. ETA=0:01:08
[32m[03/28 19:40:43 d2.evaluation.evaluator]: [0mInference done 3267/3489. 0.2388 s / img. ETA=0:01:02
[32m[03/28 19:40:48 d2.evaluation.evaluator]: [0mInference done 3288/3489. 0.2387 s / img. ETA=0:00:56
[32m[03/28 19:40:53 d2.evaluation.evaluator]: [0mInference done 3310/3489. 0.2386 s / img. ETA=0:00:50
[32m[03/28 19:40:58 d2.evaluation.evaluator]: [0mInference done 3332/3489. 0.2386 s / img. ETA=0:00:44
[32m[03/28 19:41:03 d2.evaluation.evaluator]: [0mInference done 3354/3489. 0.2385 s / img. ETA=0:00:37
[32m[03/28 19:41:08 d2.evaluation.evaluator]: [0mInference done 3376/3489. 0.2384 s / img. ETA=0:00:31
[32m[03/28 19:41:14 d2.evaluation.evaluator]: [0mInference done 3398/3489. 0.2383 s / img. ETA=0:00:25
[32m[03/28 19:41:19 d2.evaluation.evaluator]: [0mInference done 3419/3489. 0.2383 s / img. ETA=0:00:19
[32m[03/28 19:41:24 d2.evaluation.evaluator]: [0mInference done 3440/3489. 0.2382 s / img. ETA=0:00:13
[32m[03/28 19:41:29 d2.evaluation.evaluator]: [0mInference done 3461/3489. 0.2382 s / img. ETA=0:00:07
[32m[03/28 19:41:34 d2.evaluation.evaluator]: [0mInference done 3482/3489. 0.2381 s / img. ETA=0:00:01
[32m[03/28 19:41:36 d2.evaluation.evaluator]: [0mTotal inference time: 0:16:14.739728 (0.279776 s / img per device, on 1 devices)
[32m[03/28 19:41:36 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:13:49 (0.238112 s / img per device, on 1 devices)
[32m[03/28 19:41:38 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[03/28 19:41:38 d2.evaluation.coco_evaluation]: [0mSaving results to ./output/coco_instances_results.json
[32m[03/28 19:41:39 d2.evaluation.coco_evaluation]: [0mEvaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.15s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
COCOeval_opt.evaluate() finished in 1.51 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.56 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.457
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.784
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.436
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.282
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.543
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.563
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.534
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.576
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.658
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751
[32m[03/28 19:41:41 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 45.666 | 78.380 | 43.638 | 28.233 | 54.273 | 56.275 |
[32m[03/28 19:41:41 d2.evaluation.coco_evaluation]: [0mPer-category bbox AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| Cars       | 58.032 | Pedestrian | 33.300 |
Loading and preparing results...
DONE (t=1.91s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
COCOeval_opt.evaluate() finished in 2.28 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.59 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.430
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.753
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.407
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.512
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.700
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.499
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.612
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.766
[32m[03/28 19:41:51 d2.evaluation.coco_evaluation]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 43.008 | 75.262 | 40.665 | 22.178 | 51.222 | 70.000 |
[32m[03/28 19:41:51 d2.evaluation.coco_evaluation]: [0mPer-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| Cars       | 60.512 | Pedestrian | 25.503 |
[32m[03/28 19:41:51 d2.engine.defaults]: [0mEvaluation results for kittimots_val in csv format:
[32m[03/28 19:41:51 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[03/28 19:41:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/28 19:41:51 d2.evaluation.testing]: [0mcopypaste: 45.6659,78.3798,43.6382,28.2334,54.2735,56.2745
[32m[03/28 19:41:51 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[03/28 19:41:51 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/28 19:41:51 d2.evaluation.testing]: [0mcopypaste: 43.0076,75.2616,40.6652,22.1778,51.2216,69.9999
evaluated
