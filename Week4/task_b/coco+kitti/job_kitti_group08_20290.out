[32m[03/29 13:49:52 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=3, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[03/29 13:49:52 d2.data.build]: [0mRemoved 729 images with no usable annotations. 3123 images left.
[32m[03/29 13:49:52 d2.data.build]: [0mDistribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|    Cars    | 12949        | Pedestrian | 469          |
|            |              |            |              |
|   total    | 13418        |            |              |[0m
[32m[03/29 13:49:52 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[03/29 13:49:52 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/29 13:49:52 d2.data.common]: [0mSerializing 3123 elements to byte tensors and concatenating them all ...
[32m[03/29 13:49:52 d2.data.common]: [0mSerialized dataset takes 4.43 MiB
[32m[03/29 13:49:53 d2.data.build]: [0mRemoved 117 images with no usable annotations. 3372 images left.
[32m[03/29 13:49:53 d2.data.build]: [0mDistribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|    Cars    | 9960         | Pedestrian | 10725        |
|            |              |            |              |
|   total    | 20685        |            |              |[0m
[32m[03/29 13:49:53 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[03/29 13:49:53 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/29 13:49:53 d2.data.common]: [0mSerializing 3372 elements to byte tensors and concatenating them all ...
[32m[03/29 13:49:53 d2.data.common]: [0mSerialized dataset takes 6.00 MiB
[32m[03/29 13:49:53 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[03/29 13:50:02 d2.utils.events]: [0m eta: 0:00:56  iter: 19  total_loss: 2.115  loss_cls: 0.9525  loss_box_reg: 0.4359  loss_mask: 0.6638  loss_rpn_cls: 0.05098  loss_rpn_loc: 0.01667  total_val_loss: 2.235  val_loss_cls: 0.9679  val_loss_box_reg: 0.3845  val_loss_mask: 0.6775  val_loss_rpn_cls: 0.07943  val_loss_rpn_loc: 0.02618  time: 0.2012  data_time: 0.0282  lr: 0.00019981  max_mem: 2909M
[32m[03/29 13:50:10 d2.utils.events]: [0m eta: 0:00:52  iter: 39  total_loss: 1.152  loss_cls: 0.312  loss_box_reg: 0.3862  loss_mask: 0.3875  loss_rpn_cls: 0.0407  loss_rpn_loc: 0.01138  total_val_loss: 1.493  val_loss_cls: 0.3348  val_loss_box_reg: 0.4314  val_loss_mask: 0.5104  val_loss_rpn_cls: 0.08332  val_loss_rpn_loc: 0.03023  time: 0.2005  data_time: 0.0068  lr: 0.00039961  max_mem: 3022M
[32m[03/29 13:50:18 d2.utils.events]: [0m eta: 0:00:47  iter: 59  total_loss: 0.8891  loss_cls: 0.1693  loss_box_reg: 0.3468  loss_mask: 0.2568  loss_rpn_cls: 0.02002  loss_rpn_loc: 0.02442  total_val_loss: 1.29  val_loss_cls: 0.3043  val_loss_box_reg: 0.4278  val_loss_mask: 0.3749  val_loss_rpn_cls: 0.05007  val_loss_rpn_loc: 0.02522  time: 0.2019  data_time: 0.0058  lr: 0.00059941  max_mem: 3192M
[32m[03/29 13:50:26 d2.utils.events]: [0m eta: 0:00:44  iter: 79  total_loss: 0.6824  loss_cls: 0.1277  loss_box_reg: 0.3121  loss_mask: 0.2158  loss_rpn_cls: 0.01223  loss_rpn_loc: 0.01337  total_val_loss: 1.205  val_loss_cls: 0.3123  val_loss_box_reg: 0.4257  val_loss_mask: 0.465  val_loss_rpn_cls: 0.0294  val_loss_rpn_loc: 0.02776  time: 0.2030  data_time: 0.0059  lr: 0.00079921  max_mem: 3192M
[32m[03/29 13:50:34 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/29 13:50:34 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 13:50:34 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 13:50:34 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/29 13:50:34 d2.utils.events]: [0m eta: 0:00:40  iter: 99  total_loss: 0.6029  loss_cls: 0.1179  loss_box_reg: 0.1845  loss_mask: 0.2081  loss_rpn_cls: 0.01622  loss_rpn_loc: 0.01446  total_val_loss: 1.218  val_loss_cls: 0.3098  val_loss_box_reg: 0.2908  val_loss_mask: 0.3679  val_loss_rpn_cls: 0.07583  val_loss_rpn_loc: 0.03322  time: 0.2024  data_time: 0.0057  lr: 0.00099901  max_mem: 3192M
[32m[03/29 13:50:42 d2.utils.events]: [0m eta: 0:00:36  iter: 119  total_loss: 0.4692  loss_cls: 0.09972  loss_box_reg: 0.1686  loss_mask: 0.1796  loss_rpn_cls: 0.01071  loss_rpn_loc: 0.01469  total_val_loss: 0.8109  val_loss_cls: 0.188  val_loss_box_reg: 0.2908  val_loss_mask: 0.2617  val_loss_rpn_cls: 0.03507  val_loss_rpn_loc: 0.01988  time: 0.2028  data_time: 0.0063  lr: 0.0011988  max_mem: 3192M
[32m[03/29 13:50:50 d2.utils.events]: [0m eta: 0:00:32  iter: 139  total_loss: 0.3954  loss_cls: 0.07647  loss_box_reg: 0.1147  loss_mask: 0.1671  loss_rpn_cls: 0.009513  loss_rpn_loc: 0.009262  total_val_loss: 1.107  val_loss_cls: 0.2806  val_loss_box_reg: 0.3499  val_loss_mask: 0.4195  val_loss_rpn_cls: 0.03292  val_loss_rpn_loc: 0.02978  time: 0.2024  data_time: 0.0057  lr: 0.0013986  max_mem: 3192M
[32m[03/29 13:50:58 d2.utils.events]: [0m eta: 0:00:28  iter: 159  total_loss: 0.4379  loss_cls: 0.09504  loss_box_reg: 0.1401  loss_mask: 0.182  loss_rpn_cls: 0.008811  loss_rpn_loc: 0.01389  total_val_loss: 0.9901  val_loss_cls: 0.2496  val_loss_box_reg: 0.3508  val_loss_mask: 0.3153  val_loss_rpn_cls: 0.02804  val_loss_rpn_loc: 0.02552  time: 0.2031  data_time: 0.0064  lr: 0.0015984  max_mem: 3192M
[32m[03/29 13:51:06 d2.utils.events]: [0m eta: 0:00:24  iter: 179  total_loss: 0.4144  loss_cls: 0.1154  loss_box_reg: 0.1389  loss_mask: 0.1359  loss_rpn_cls: 0.0092  loss_rpn_loc: 0.01594  total_val_loss: 0.7534  val_loss_cls: 0.1657  val_loss_box_reg: 0.2651  val_loss_mask: 0.2942  val_loss_rpn_cls: 0.02792  val_loss_rpn_loc: 0.02426  time: 0.2042  data_time: 0.0057  lr: 0.0017982  max_mem: 3192M
[32m[03/29 13:51:14 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/29 13:51:14 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 13:51:14 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 13:51:14 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/29 13:51:15 d2.utils.events]: [0m eta: 0:00:20  iter: 199  total_loss: 0.5429  loss_cls: 0.1202  loss_box_reg: 0.1844  loss_mask: 0.2199  loss_rpn_cls: 0.00811  loss_rpn_loc: 0.01893  total_val_loss: 1.117  val_loss_cls: 0.2617  val_loss_box_reg: 0.3494  val_loss_mask: 0.3707  val_loss_rpn_cls: 0.02936  val_loss_rpn_loc: 0.03412  time: 0.2046  data_time: 0.0058  lr: 0.001998  max_mem: 3192M
[32m[03/29 13:51:23 d2.utils.events]: [0m eta: 0:00:16  iter: 219  total_loss: 0.588  loss_cls: 0.09779  loss_box_reg: 0.1816  loss_mask: 0.1966  loss_rpn_cls: 0.008775  loss_rpn_loc: 0.01813  total_val_loss: 0.8432  val_loss_cls: 0.1918  val_loss_box_reg: 0.2761  val_loss_mask: 0.2729  val_loss_rpn_cls: 0.02792  val_loss_rpn_loc: 0.02123  time: 0.2055  data_time: 0.0061  lr: 0.0021978  max_mem: 3193M
[32m[03/29 13:51:31 d2.utils.events]: [0m eta: 0:00:12  iter: 239  total_loss: 0.5009  loss_cls: 0.0951  loss_box_reg: 0.1621  loss_mask: 0.2187  loss_rpn_cls: 0.0118  loss_rpn_loc: 0.01814  total_val_loss: 0.7046  val_loss_cls: 0.1635  val_loss_box_reg: 0.2405  val_loss_mask: 0.269  val_loss_rpn_cls: 0.01866  val_loss_rpn_loc: 0.0292  time: 0.2053  data_time: 0.0060  lr: 0.0023976  max_mem: 3193M
[32m[03/29 13:51:39 d2.utils.events]: [0m eta: 0:00:08  iter: 259  total_loss: 0.3738  loss_cls: 0.06393  loss_box_reg: 0.1227  loss_mask: 0.1686  loss_rpn_cls: 0.00657  loss_rpn_loc: 0.01069  total_val_loss: 0.9736  val_loss_cls: 0.1864  val_loss_box_reg: 0.2952  val_loss_mask: 0.3135  val_loss_rpn_cls: 0.03496  val_loss_rpn_loc: 0.03887  time: 0.2054  data_time: 0.0056  lr: 0.0025974  max_mem: 3193M
[32m[03/29 13:51:47 d2.utils.events]: [0m eta: 0:00:04  iter: 279  total_loss: 0.4272  loss_cls: 0.09322  loss_box_reg: 0.1352  loss_mask: 0.178  loss_rpn_cls: 0.006542  loss_rpn_loc: 0.01368  total_val_loss: 0.9434  val_loss_cls: 0.2114  val_loss_box_reg: 0.358  val_loss_mask: 0.2717  val_loss_rpn_cls: 0.02143  val_loss_rpn_loc: 0.03637  time: 0.2055  data_time: 0.0059  lr: 0.0027972  max_mem: 3193M
[32m[03/29 13:51:56 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/29 13:51:56 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 13:51:56 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 13:51:56 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/29 13:51:56 d2.utils.events]: [0m eta: 0:00:00  iter: 299  total_loss: 0.4569  loss_cls: 0.1025  loss_box_reg: 0.1698  loss_mask: 0.1663  loss_rpn_cls: 0.007526  loss_rpn_loc: 0.01787  total_val_loss: 0.8955  val_loss_cls: 0.1997  val_loss_box_reg: 0.3391  val_loss_mask: 0.3369  val_loss_rpn_cls: 0.03094  val_loss_rpn_loc: 0.03379  time: 0.2057  data_time: 0.0057  lr: 0.002997  max_mem: 3193M
[32m[03/29 13:51:56 d2.engine.hooks]: [0mOverall training speed: 298 iterations in 0:01:01 (0.2058 s / it)
[32m[03/29 13:51:56 d2.engine.hooks]: [0mTotal training time: 0:02:01 (0:00:59 on hooks)
[32m[03/29 13:51:57 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/29 13:51:57 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 13:51:57 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 13:51:57 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[5m[31mWARNING[0m [32m[03/29 13:51:57 d2.evaluation.coco_evaluation]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass tasks in directly
[32m[03/29 13:51:57 d2.evaluation.coco_evaluation]: [0m'kittimots_test' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...
[5m[31mWARNING[0m [32m[03/29 13:51:57 d2.data.datasets.coco]: [0mUsing previously cached COCO format annotations at './output/kittimots_test_coco_format.json'. You need to clear the cache file if your dataset has been modified.
[32m[03/29 13:51:57 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[32m[03/29 13:51:57 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 13:51:57 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[32m[03/29 13:51:57 d2.evaluation.evaluator]: [0mStart inference on 3489 images
[32m[03/29 13:51:58 d2.evaluation.evaluator]: [0mInference done 11/3489. 0.0486 s / img. ETA=0:03:27
[32m[03/29 13:52:03 d2.evaluation.evaluator]: [0mInference done 97/3489. 0.0492 s / img. ETA=0:03:19
[32m[03/29 13:52:08 d2.evaluation.evaluator]: [0mInference done 154/3489. 0.0519 s / img. ETA=0:03:54
[32m[03/29 13:52:13 d2.evaluation.evaluator]: [0mInference done 200/3489. 0.0541 s / img. ETA=0:04:21
[32m[03/29 13:52:18 d2.evaluation.evaluator]: [0mInference done 272/3489. 0.0539 s / img. ETA=0:04:07
[32m[03/29 13:52:23 d2.evaluation.evaluator]: [0mInference done 334/3489. 0.0541 s / img. ETA=0:04:05
[32m[03/29 13:52:29 d2.evaluation.evaluator]: [0mInference done 391/3489. 0.0545 s / img. ETA=0:04:06
[32m[03/29 13:52:34 d2.evaluation.evaluator]: [0mInference done 459/3489. 0.0545 s / img. ETA=0:03:58
[32m[03/29 13:52:39 d2.evaluation.evaluator]: [0mInference done 539/3489. 0.0541 s / img. ETA=0:03:44
[32m[03/29 13:52:44 d2.evaluation.evaluator]: [0mInference done 606/3489. 0.0541 s / img. ETA=0:03:39
[32m[03/29 13:52:49 d2.evaluation.evaluator]: [0mInference done 663/3489. 0.0545 s / img. ETA=0:03:38
[32m[03/29 13:52:54 d2.evaluation.evaluator]: [0mInference done 737/3489. 0.0543 s / img. ETA=0:03:29
[32m[03/29 13:52:59 d2.evaluation.evaluator]: [0mInference done 823/3489. 0.0540 s / img. ETA=0:03:18
[32m[03/29 13:53:04 d2.evaluation.evaluator]: [0mInference done 907/3489. 0.0537 s / img. ETA=0:03:08
[32m[03/29 13:53:09 d2.evaluation.evaluator]: [0mInference done 954/3489. 0.0540 s / img. ETA=0:03:09
[32m[03/29 13:53:14 d2.evaluation.evaluator]: [0mInference done 996/3489. 0.0545 s / img. ETA=0:03:11
[32m[03/29 13:53:19 d2.evaluation.evaluator]: [0mInference done 1032/3489. 0.0550 s / img. ETA=0:03:14
[32m[03/29 13:53:24 d2.evaluation.evaluator]: [0mInference done 1077/3489. 0.0555 s / img. ETA=0:03:14
[32m[03/29 13:53:29 d2.evaluation.evaluator]: [0mInference done 1134/3489. 0.0556 s / img. ETA=0:03:10
[32m[03/29 13:53:34 d2.evaluation.evaluator]: [0mInference done 1186/3489. 0.0557 s / img. ETA=0:03:07
[32m[03/29 13:53:39 d2.evaluation.evaluator]: [0mInference done 1249/3489. 0.0556 s / img. ETA=0:03:02
[32m[03/29 13:53:44 d2.evaluation.evaluator]: [0mInference done 1341/3489. 0.0552 s / img. ETA=0:02:51
[32m[03/29 13:53:49 d2.evaluation.evaluator]: [0mInference done 1424/3489. 0.0549 s / img. ETA=0:02:42
[32m[03/29 13:53:54 d2.evaluation.evaluator]: [0mInference done 1506/3489. 0.0547 s / img. ETA=0:02:33
[32m[03/29 13:53:59 d2.evaluation.evaluator]: [0mInference done 1589/3489. 0.0545 s / img. ETA=0:02:25
[32m[03/29 13:54:04 d2.evaluation.evaluator]: [0mInference done 1657/3489. 0.0544 s / img. ETA=0:02:20
[32m[03/29 13:54:09 d2.evaluation.evaluator]: [0mInference done 1715/3489. 0.0545 s / img. ETA=0:02:16
[32m[03/29 13:54:15 d2.evaluation.evaluator]: [0mInference done 1769/3489. 0.0546 s / img. ETA=0:02:13
[32m[03/29 13:54:20 d2.evaluation.evaluator]: [0mInference done 1803/3489. 0.0548 s / img. ETA=0:02:12
[32m[03/29 13:54:25 d2.evaluation.evaluator]: [0mInference done 1839/3489. 0.0551 s / img. ETA=0:02:12
[32m[03/29 13:54:30 d2.evaluation.evaluator]: [0mInference done 1880/3489. 0.0553 s / img. ETA=0:02:10
[32m[03/29 13:54:35 d2.evaluation.evaluator]: [0mInference done 1920/3489. 0.0555 s / img. ETA=0:02:08
[32m[03/29 13:54:40 d2.evaluation.evaluator]: [0mInference done 1959/3489. 0.0556 s / img. ETA=0:02:06
[32m[03/29 13:54:45 d2.evaluation.evaluator]: [0mInference done 2004/3489. 0.0558 s / img. ETA=0:02:04
[32m[03/29 13:54:50 d2.evaluation.evaluator]: [0mInference done 2062/3489. 0.0557 s / img. ETA=0:01:59
[32m[03/29 13:54:55 d2.evaluation.evaluator]: [0mInference done 2111/3489. 0.0558 s / img. ETA=0:01:55
[32m[03/29 13:55:00 d2.evaluation.evaluator]: [0mInference done 2157/3489. 0.0559 s / img. ETA=0:01:52
[32m[03/29 13:55:05 d2.evaluation.evaluator]: [0mInference done 2197/3489. 0.0560 s / img. ETA=0:01:50
[32m[03/29 13:55:10 d2.evaluation.evaluator]: [0mInference done 2239/3489. 0.0562 s / img. ETA=0:01:47
[32m[03/29 13:55:15 d2.evaluation.evaluator]: [0mInference done 2287/3489. 0.0563 s / img. ETA=0:01:43
[32m[03/29 13:55:20 d2.evaluation.evaluator]: [0mInference done 2338/3489. 0.0563 s / img. ETA=0:01:39
[32m[03/29 13:55:25 d2.evaluation.evaluator]: [0mInference done 2397/3489. 0.0562 s / img. ETA=0:01:34
[32m[03/29 13:55:30 d2.evaluation.evaluator]: [0mInference done 2451/3489. 0.0562 s / img. ETA=0:01:30
[32m[03/29 13:55:35 d2.evaluation.evaluator]: [0mInference done 2498/3489. 0.0564 s / img. ETA=0:01:26
[32m[03/29 13:55:40 d2.evaluation.evaluator]: [0mInference done 2566/3489. 0.0563 s / img. ETA=0:01:20
[32m[03/29 13:55:45 d2.evaluation.evaluator]: [0mInference done 2646/3489. 0.0561 s / img. ETA=0:01:12
[32m[03/29 13:55:50 d2.evaluation.evaluator]: [0mInference done 2716/3489. 0.0560 s / img. ETA=0:01:06
[32m[03/29 13:55:55 d2.evaluation.evaluator]: [0mInference done 2789/3489. 0.0559 s / img. ETA=0:00:59
[32m[03/29 13:56:00 d2.evaluation.evaluator]: [0mInference done 2859/3489. 0.0558 s / img. ETA=0:00:53
[32m[03/29 13:56:05 d2.evaluation.evaluator]: [0mInference done 2938/3489. 0.0557 s / img. ETA=0:00:46
[32m[03/29 13:56:11 d2.evaluation.evaluator]: [0mInference done 3018/3489. 0.0555 s / img. ETA=0:00:39
[32m[03/29 13:56:16 d2.evaluation.evaluator]: [0mInference done 3090/3489. 0.0554 s / img. ETA=0:00:33
[32m[03/29 13:56:21 d2.evaluation.evaluator]: [0mInference done 3164/3489. 0.0553 s / img. ETA=0:00:27
[32m[03/29 13:56:26 d2.evaluation.evaluator]: [0mInference done 3239/3489. 0.0553 s / img. ETA=0:00:20
[32m[03/29 13:56:31 d2.evaluation.evaluator]: [0mInference done 3322/3489. 0.0552 s / img. ETA=0:00:13
[32m[03/29 13:56:36 d2.evaluation.evaluator]: [0mInference done 3413/3489. 0.0550 s / img. ETA=0:00:06
[32m[03/29 13:56:40 d2.evaluation.evaluator]: [0mTotal inference time: 0:04:42.331813 (0.081037 s / img per device, on 1 devices)
[32m[03/29 13:56:40 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:03:11 (0.054883 s / img per device, on 1 devices)
[32m[03/29 13:56:42 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[03/29 13:56:42 d2.evaluation.coco_evaluation]: [0mSaving results to ./output/coco_instances_results.json
[32m[03/29 13:56:43 d2.evaluation.coco_evaluation]: [0mEvaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.11s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
COCOeval_opt.evaluate() finished in 1.50 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.41 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.503
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.809
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.332
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.586
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.690
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.559
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.593
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.427
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.667
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.810
[32m[03/29 13:56:45 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 50.295 | 80.948 | 54.573 | 33.248 | 58.642 | 68.973 |
[32m[03/29 13:56:45 d2.evaluation.coco_evaluation]: [0mPer-category bbox AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| Cars       | 59.523 | Pedestrian | 41.066 |
Loading and preparing results...
DONE (t=1.53s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
COCOeval_opt.evaluate() finished in 2.04 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.43 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.788
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.527
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.575
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.399
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.651
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.817
[32m[03/29 13:56:52 d2.evaluation.coco_evaluation]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 49.691 | 78.850 | 52.662 | 28.409 | 58.183 | 76.177 |
[32m[03/29 13:56:52 d2.evaluation.coco_evaluation]: [0mPer-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| Cars       | 63.647 | Pedestrian | 35.735 |
[32m[03/29 13:56:52 d2.engine.defaults]: [0mEvaluation results for kittimots_test in csv format:
[32m[03/29 13:56:52 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[03/29 13:56:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/29 13:56:52 d2.evaluation.testing]: [0mcopypaste: 50.2949,80.9482,54.5732,33.2483,58.6420,68.9730
[32m[03/29 13:56:52 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[03/29 13:56:52 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/29 13:56:52 d2.evaluation.testing]: [0mcopypaste: 49.6911,78.8498,52.6622,28.4091,58.1832,76.1775
evaluated
