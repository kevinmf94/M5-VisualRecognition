[32m[03/29 17:09:46 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=3, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (mask_fcn4): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv_relu): ReLU()
      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[03/29 17:09:46 d2.data.build]: [0mRemoved 729 images with no usable annotations. 3123 images left.
[32m[03/29 17:09:46 d2.data.build]: [0mDistribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|    Cars    | 12949        | Pedestrian | 469          |
|            |              |            |              |
|   total    | 13418        |            |              |[0m
[32m[03/29 17:09:46 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800, 832, 864, 896, 928, 960, 992, 1024), max_size=2048, sample_style='choice'), RandomFlip()]
[32m[03/29 17:09:46 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/29 17:09:46 d2.data.common]: [0mSerializing 3123 elements to byte tensors and concatenating them all ...
[32m[03/29 17:09:46 d2.data.common]: [0mSerialized dataset takes 4.43 MiB
[32m[03/29 17:09:46 d2.data.build]: [0mRemoved 117 images with no usable annotations. 3372 images left.
[32m[03/29 17:09:46 d2.data.build]: [0mDistribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|    Cars    | 9960         | Pedestrian | 10725        |
|            |              |            |              |
|   total    | 20685        |            |              |[0m
[32m[03/29 17:09:46 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800, 832, 864, 896, 928, 960, 992, 1024), max_size=2048, sample_style='choice'), RandomFlip()]
[32m[03/29 17:09:46 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[03/29 17:09:46 d2.data.common]: [0mSerializing 3372 elements to byte tensors and concatenating them all ...
[32m[03/29 17:09:47 d2.data.common]: [0mSerialized dataset takes 6.00 MiB
[32m[03/29 17:09:47 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[03/29 17:10:02 d2.utils.events]: [0m eta: 0:01:26  iter: 19  total_loss: 2.283  loss_cls: 0.9958  loss_box_reg: 0.5032  loss_mask: 0.6807  loss_rpn_cls: 0.01025  loss_rpn_loc: 0.01295  total_val_loss: 2.38  val_loss_cls: 0.9994  val_loss_box_reg: 0.6114  val_loss_mask: 0.686  val_loss_rpn_cls: 0.03  val_loss_rpn_loc: 0.02037  time: 0.3169  data_time: 0.0320  lr: 0.00019981  max_mem: 5575M
[32m[03/29 17:10:16 d2.utils.events]: [0m eta: 0:01:22  iter: 39  total_loss: 1.518  loss_cls: 0.3915  loss_box_reg: 0.6216  loss_mask: 0.4517  loss_rpn_cls: 0.00941  loss_rpn_loc: 0.00958  total_val_loss: 1.705  val_loss_cls: 0.4761  val_loss_box_reg: 0.6445  val_loss_mask: 0.563  val_loss_rpn_cls: 0.01473  val_loss_rpn_loc: 0.02356  time: 0.3221  data_time: 0.0078  lr: 0.00039961  max_mem: 5575M
[32m[03/29 17:10:30 d2.utils.events]: [0m eta: 0:01:18  iter: 59  total_loss: 1.07  loss_cls: 0.2568  loss_box_reg: 0.5061  loss_mask: 0.2544  loss_rpn_cls: 0.008921  loss_rpn_loc: 0.01061  total_val_loss: 1.474  val_loss_cls: 0.3559  val_loss_box_reg: 0.5611  val_loss_mask: 0.433  val_loss_rpn_cls: 0.03033  val_loss_rpn_loc: 0.027  time: 0.3265  data_time: 0.0072  lr: 0.00059941  max_mem: 5577M
[32m[03/29 17:10:45 d2.utils.events]: [0m eta: 0:01:11  iter: 79  total_loss: 0.7572  loss_cls: 0.1737  loss_box_reg: 0.363  loss_mask: 0.1731  loss_rpn_cls: 0.009731  loss_rpn_loc: 0.01024  total_val_loss: 1.524  val_loss_cls: 0.3899  val_loss_box_reg: 0.5548  val_loss_mask: 0.4802  val_loss_rpn_cls: 0.04439  val_loss_rpn_loc: 0.03131  time: 0.3273  data_time: 0.0071  lr: 0.00079921  max_mem: 5577M
[32m[03/29 17:10:59 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[32m[03/29 17:10:59 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 17:10:59 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 17:10:59 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/29 17:10:59 d2.utils.events]: [0m eta: 0:01:05  iter: 99  total_loss: 0.5684  loss_cls: 0.1201  loss_box_reg: 0.2347  loss_mask: 0.1714  loss_rpn_cls: 0.004066  loss_rpn_loc: 0.008921  total_val_loss: 1.211  val_loss_cls: 0.3182  val_loss_box_reg: 0.3914  val_loss_mask: 0.4068  val_loss_rpn_cls: 0.03696  val_loss_rpn_loc: 0.02709  time: 0.3278  data_time: 0.0090  lr: 0.00099901  max_mem: 5583M
[32m[03/29 17:11:13 d2.utils.events]: [0m eta: 0:00:58  iter: 119  total_loss: 0.4037  loss_cls: 0.09686  loss_box_reg: 0.1356  loss_mask: 0.1364  loss_rpn_cls: 0.005531  loss_rpn_loc: 0.006757  total_val_loss: 1.079  val_loss_cls: 0.2482  val_loss_box_reg: 0.3322  val_loss_mask: 0.3404  val_loss_rpn_cls: 0.02912  val_loss_rpn_loc: 0.02489  time: 0.3257  data_time: 0.0073  lr: 0.0011988  max_mem: 5583M
[32m[03/29 17:11:27 d2.utils.events]: [0m eta: 0:00:51  iter: 139  total_loss: 0.4041  loss_cls: 0.09621  loss_box_reg: 0.1432  loss_mask: 0.1376  loss_rpn_cls: 0.005029  loss_rpn_loc: 0.01172  total_val_loss: 0.8504  val_loss_cls: 0.2202  val_loss_box_reg: 0.297  val_loss_mask: 0.2964  val_loss_rpn_cls: 0.02327  val_loss_rpn_loc: 0.0148  time: 0.3237  data_time: 0.0085  lr: 0.0013986  max_mem: 5583M
[32m[03/29 17:11:41 d2.utils.events]: [0m eta: 0:00:45  iter: 159  total_loss: 0.4109  loss_cls: 0.1056  loss_box_reg: 0.1525  loss_mask: 0.1448  loss_rpn_cls: 0.004405  loss_rpn_loc: 0.01127  total_val_loss: 0.9216  val_loss_cls: 0.2656  val_loss_box_reg: 0.3136  val_loss_mask: 0.2858  val_loss_rpn_cls: 0.01429  val_loss_rpn_loc: 0.02174  time: 0.3237  data_time: 0.0072  lr: 0.0015984  max_mem: 5583M
[32m[03/29 17:11:55 d2.utils.events]: [0m eta: 0:00:38  iter: 179  total_loss: 0.4243  loss_cls: 0.1053  loss_box_reg: 0.1463  loss_mask: 0.1311  loss_rpn_cls: 0.0004089  loss_rpn_loc: 0.008555  total_val_loss: 1.228  val_loss_cls: 0.326  val_loss_box_reg: 0.3988  val_loss_mask: 0.3603  val_loss_rpn_cls: 0.01817  val_loss_rpn_loc: 0.02203  time: 0.3235  data_time: 0.0071  lr: 0.0017982  max_mem: 5583M
[32m[03/29 17:12:09 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[32m[03/29 17:12:09 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 17:12:09 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 17:12:09 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/29 17:12:10 d2.utils.events]: [0m eta: 0:00:32  iter: 199  total_loss: 0.3864  loss_cls: 0.09281  loss_box_reg: 0.1349  loss_mask: 0.1248  loss_rpn_cls: 0.001002  loss_rpn_loc: 0.007545  total_val_loss: 1.163  val_loss_cls: 0.3181  val_loss_box_reg: 0.4063  val_loss_mask: 0.3213  val_loss_rpn_cls: 0.02681  val_loss_rpn_loc: 0.02319  time: 0.3234  data_time: 0.0072  lr: 0.001998  max_mem: 5583M
[32m[03/29 17:12:24 d2.utils.events]: [0m eta: 0:00:25  iter: 219  total_loss: 0.2865  loss_cls: 0.07721  loss_box_reg: 0.1039  loss_mask: 0.1147  loss_rpn_cls: 0.0003782  loss_rpn_loc: 0.004901  total_val_loss: 1.076  val_loss_cls: 0.2991  val_loss_box_reg: 0.4325  val_loss_mask: 0.2879  val_loss_rpn_cls: 0.01798  val_loss_rpn_loc: 0.0275  time: 0.3232  data_time: 0.0074  lr: 0.0021978  max_mem: 5583M
[32m[03/29 17:12:38 d2.utils.events]: [0m eta: 0:00:19  iter: 239  total_loss: 0.4337  loss_cls: 0.1238  loss_box_reg: 0.1246  loss_mask: 0.1515  loss_rpn_cls: 0.02383  loss_rpn_loc: 0.01204  total_val_loss: 0.832  val_loss_cls: 0.1846  val_loss_box_reg: 0.3065  val_loss_mask: 0.2612  val_loss_rpn_cls: 0.03132  val_loss_rpn_loc: 0.02209  time: 0.3234  data_time: 0.0078  lr: 0.0023976  max_mem: 5583M
[32m[03/29 17:12:52 d2.utils.events]: [0m eta: 0:00:12  iter: 259  total_loss: 0.3593  loss_cls: 0.07745  loss_box_reg: 0.1339  loss_mask: 0.1464  loss_rpn_cls: 0.005494  loss_rpn_loc: 0.01076  total_val_loss: 1.147  val_loss_cls: 0.2735  val_loss_box_reg: 0.4104  val_loss_mask: 0.3668  val_loss_rpn_cls: 0.021  val_loss_rpn_loc: 0.02834  time: 0.3225  data_time: 0.0071  lr: 0.0025974  max_mem: 5583M
[32m[03/29 17:13:06 d2.utils.events]: [0m eta: 0:00:06  iter: 279  total_loss: 0.4008  loss_cls: 0.08483  loss_box_reg: 0.1289  loss_mask: 0.1563  loss_rpn_cls: 0.004901  loss_rpn_loc: 0.01211  total_val_loss: 1.007  val_loss_cls: 0.238  val_loss_box_reg: 0.299  val_loss_mask: 0.3204  val_loss_rpn_cls: 0.02791  val_loss_rpn_loc: 0.02473  time: 0.3226  data_time: 0.0075  lr: 0.0027972  max_mem: 5583M
[32m[03/29 17:13:22 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[32m[03/29 17:13:22 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 17:13:22 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 17:13:22 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[32m[03/29 17:13:22 d2.utils.events]: [0m eta: 0:00:00  iter: 299  total_loss: 0.3896  loss_cls: 0.09179  loss_box_reg: 0.149  loss_mask: 0.1262  loss_rpn_cls: 0.005805  loss_rpn_loc: 0.01238  total_val_loss: 0.8043  val_loss_cls: 0.1671  val_loss_box_reg: 0.2259  val_loss_mask: 0.2746  val_loss_rpn_cls: 0.01158  val_loss_rpn_loc: 0.01508  time: 0.3235  data_time: 0.0078  lr: 0.002997  max_mem: 5583M
[32m[03/29 17:13:22 d2.engine.hooks]: [0mOverall training speed: 298 iterations in 0:01:36 (0.3235 s / it)
[32m[03/29 17:13:22 d2.engine.hooks]: [0mTotal training time: 0:03:33 (0:01:56 on hooks)
[32m[03/29 17:13:22 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[32m[03/29 17:13:22 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 17:13:22 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[5m[31mWARNING[0m [32m[03/29 17:13:22 d2.engine.defaults]: [0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.
[5m[31mWARNING[0m [32m[03/29 17:13:22 d2.evaluation.coco_evaluation]: [0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass tasks in directly
[32m[03/29 17:13:22 d2.evaluation.coco_evaluation]: [0m'kittimots_test' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...
[32m[03/29 17:13:22 d2.data.datasets.coco]: [0mConverting annotations of dataset 'kittimots_test' to COCO format ...)
[32m[03/29 17:13:23 d2.data.datasets.coco]: [0mConverting dataset dicts into COCO format
[32m[03/29 17:13:23 d2.data.datasets.coco]: [0mConversion finished, #images: 3489, #annotations: 20685
[32m[03/29 17:13:23 d2.data.datasets.coco]: [0mCaching COCO format annotations at './output/kittimots_test_coco_format.json' ...
[32m[03/29 17:13:24 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[32m[03/29 17:13:24 d2.data.common]: [0mSerializing 3489 elements to byte tensors and concatenating them all ...
[32m[03/29 17:13:24 d2.data.common]: [0mSerialized dataset takes 6.03 MiB
[32m[03/29 17:13:24 d2.evaluation.evaluator]: [0mStart inference on 3489 images
[32m[03/29 17:13:26 d2.evaluation.evaluator]: [0mInference done 11/3489. 0.0850 s / img. ETA=0:05:28
[32m[03/29 17:13:31 d2.evaluation.evaluator]: [0mInference done 64/3489. 0.0850 s / img. ETA=0:05:24
[32m[03/29 17:13:36 d2.evaluation.evaluator]: [0mInference done 116/3489. 0.0851 s / img. ETA=0:05:23
[32m[03/29 17:13:41 d2.evaluation.evaluator]: [0mInference done 157/3489. 0.0869 s / img. ETA=0:05:43
[32m[03/29 17:13:46 d2.evaluation.evaluator]: [0mInference done 198/3489. 0.0881 s / img. ETA=0:05:52
[32m[03/29 17:13:51 d2.evaluation.evaluator]: [0mInference done 249/3489. 0.0878 s / img. ETA=0:05:42
[32m[03/29 17:13:56 d2.evaluation.evaluator]: [0mInference done 299/3489. 0.0874 s / img. ETA=0:05:34
[32m[03/29 17:14:01 d2.evaluation.evaluator]: [0mInference done 342/3489. 0.0878 s / img. ETA=0:05:34
[32m[03/29 17:14:06 d2.evaluation.evaluator]: [0mInference done 386/3489. 0.0881 s / img. ETA=0:05:33
[32m[03/29 17:14:11 d2.evaluation.evaluator]: [0mInference done 434/3489. 0.0880 s / img. ETA=0:05:27
[32m[03/29 17:14:16 d2.evaluation.evaluator]: [0mInference done 487/3489. 0.0877 s / img. ETA=0:05:17
[32m[03/29 17:14:21 d2.evaluation.evaluator]: [0mInference done 541/3489. 0.0875 s / img. ETA=0:05:08
[32m[03/29 17:14:26 d2.evaluation.evaluator]: [0mInference done 590/3489. 0.0875 s / img. ETA=0:05:03
[32m[03/29 17:14:31 d2.evaluation.evaluator]: [0mInference done 637/3489. 0.0876 s / img. ETA=0:04:59
[32m[03/29 17:14:36 d2.evaluation.evaluator]: [0mInference done 688/3489. 0.0874 s / img. ETA=0:04:52
[32m[03/29 17:14:41 d2.evaluation.evaluator]: [0mInference done 740/3489. 0.0873 s / img. ETA=0:04:45
[32m[03/29 17:14:46 d2.evaluation.evaluator]: [0mInference done 795/3489. 0.0871 s / img. ETA=0:04:37
[32m[03/29 17:14:51 d2.evaluation.evaluator]: [0mInference done 851/3489. 0.0870 s / img. ETA=0:04:29
[32m[03/29 17:14:57 d2.evaluation.evaluator]: [0mInference done 905/3489. 0.0869 s / img. ETA=0:04:22
[32m[03/29 17:15:02 d2.evaluation.evaluator]: [0mInference done 945/3489. 0.0873 s / img. ETA=0:04:21
[32m[03/29 17:15:07 d2.evaluation.evaluator]: [0mInference done 985/3489. 0.0876 s / img. ETA=0:04:19
[32m[03/29 17:15:12 d2.evaluation.evaluator]: [0mInference done 1023/3489. 0.0878 s / img. ETA=0:04:18
[32m[03/29 17:15:17 d2.evaluation.evaluator]: [0mInference done 1063/3489. 0.0880 s / img. ETA=0:04:16
[32m[03/29 17:15:22 d2.evaluation.evaluator]: [0mInference done 1107/3489. 0.0881 s / img. ETA=0:04:12
[32m[03/29 17:15:27 d2.evaluation.evaluator]: [0mInference done 1150/3489. 0.0883 s / img. ETA=0:04:09
[32m[03/29 17:15:32 d2.evaluation.evaluator]: [0mInference done 1192/3489. 0.0884 s / img. ETA=0:04:05
[32m[03/29 17:15:37 d2.evaluation.evaluator]: [0mInference done 1237/3489. 0.0885 s / img. ETA=0:04:01
[32m[03/29 17:15:42 d2.evaluation.evaluator]: [0mInference done 1292/3489. 0.0883 s / img. ETA=0:03:53
[32m[03/29 17:15:47 d2.evaluation.evaluator]: [0mInference done 1348/3489. 0.0882 s / img. ETA=0:03:46
[32m[03/29 17:15:52 d2.evaluation.evaluator]: [0mInference done 1402/3489. 0.0881 s / img. ETA=0:03:39
[32m[03/29 17:15:57 d2.evaluation.evaluator]: [0mInference done 1455/3489. 0.0880 s / img. ETA=0:03:33
[32m[03/29 17:16:02 d2.evaluation.evaluator]: [0mInference done 1509/3489. 0.0879 s / img. ETA=0:03:26
[32m[03/29 17:16:07 d2.evaluation.evaluator]: [0mInference done 1562/3489. 0.0879 s / img. ETA=0:03:20
[32m[03/29 17:16:12 d2.evaluation.evaluator]: [0mInference done 1612/3489. 0.0879 s / img. ETA=0:03:15
[32m[03/29 17:16:17 d2.evaluation.evaluator]: [0mInference done 1660/3489. 0.0879 s / img. ETA=0:03:10
[32m[03/29 17:16:22 d2.evaluation.evaluator]: [0mInference done 1703/3489. 0.0880 s / img. ETA=0:03:06
[32m[03/29 17:16:27 d2.evaluation.evaluator]: [0mInference done 1744/3489. 0.0881 s / img. ETA=0:03:03
[32m[03/29 17:16:33 d2.evaluation.evaluator]: [0mInference done 1779/3489. 0.0883 s / img. ETA=0:03:00
[32m[03/29 17:16:38 d2.evaluation.evaluator]: [0mInference done 1808/3489. 0.0885 s / img. ETA=0:02:59
[32m[03/29 17:16:43 d2.evaluation.evaluator]: [0mInference done 1841/3489. 0.0887 s / img. ETA=0:02:57
[32m[03/29 17:16:48 d2.evaluation.evaluator]: [0mInference done 1874/3489. 0.0889 s / img. ETA=0:02:55
[32m[03/29 17:16:53 d2.evaluation.evaluator]: [0mInference done 1909/3489. 0.0890 s / img. ETA=0:02:52
[32m[03/29 17:16:58 d2.evaluation.evaluator]: [0mInference done 1948/3489. 0.0891 s / img. ETA=0:02:48
[32m[03/29 17:17:03 d2.evaluation.evaluator]: [0mInference done 1985/3489. 0.0892 s / img. ETA=0:02:45
[32m[03/29 17:17:08 d2.evaluation.evaluator]: [0mInference done 2031/3489. 0.0892 s / img. ETA=0:02:40
[32m[03/29 17:17:13 d2.evaluation.evaluator]: [0mInference done 2072/3489. 0.0892 s / img. ETA=0:02:36
[32m[03/29 17:17:18 d2.evaluation.evaluator]: [0mInference done 2110/3489. 0.0893 s / img. ETA=0:02:32
[32m[03/29 17:17:23 d2.evaluation.evaluator]: [0mInference done 2150/3489. 0.0894 s / img. ETA=0:02:28
[32m[03/29 17:17:28 d2.evaluation.evaluator]: [0mInference done 2189/3489. 0.0895 s / img. ETA=0:02:24
[32m[03/29 17:17:33 d2.evaluation.evaluator]: [0mInference done 2227/3489. 0.0896 s / img. ETA=0:02:20
[32m[03/29 17:17:38 d2.evaluation.evaluator]: [0mInference done 2268/3489. 0.0896 s / img. ETA=0:02:16
[32m[03/29 17:17:43 d2.evaluation.evaluator]: [0mInference done 2310/3489. 0.0897 s / img. ETA=0:02:12
[32m[03/29 17:17:48 d2.evaluation.evaluator]: [0mInference done 2351/3489. 0.0897 s / img. ETA=0:02:07
[32m[03/29 17:17:54 d2.evaluation.evaluator]: [0mInference done 2399/3489. 0.0896 s / img. ETA=0:02:02
[32m[03/29 17:17:59 d2.evaluation.evaluator]: [0mInference done 2444/3489. 0.0896 s / img. ETA=0:01:57
[32m[03/29 17:18:04 d2.evaluation.evaluator]: [0mInference done 2487/3489. 0.0897 s / img. ETA=0:01:52
[32m[03/29 17:18:09 d2.evaluation.evaluator]: [0mInference done 2539/3489. 0.0896 s / img. ETA=0:01:46
[32m[03/29 17:18:14 d2.evaluation.evaluator]: [0mInference done 2590/3489. 0.0895 s / img. ETA=0:01:40
[32m[03/29 17:18:19 d2.evaluation.evaluator]: [0mInference done 2643/3489. 0.0894 s / img. ETA=0:01:34
[32m[03/29 17:18:24 d2.evaluation.evaluator]: [0mInference done 2693/3489. 0.0894 s / img. ETA=0:01:28
[32m[03/29 17:18:29 d2.evaluation.evaluator]: [0mInference done 2743/3489. 0.0893 s / img. ETA=0:01:22
[32m[03/29 17:18:34 d2.evaluation.evaluator]: [0mInference done 2793/3489. 0.0893 s / img. ETA=0:01:17
[32m[03/29 17:18:39 d2.evaluation.evaluator]: [0mInference done 2842/3489. 0.0893 s / img. ETA=0:01:11
[32m[03/29 17:18:44 d2.evaluation.evaluator]: [0mInference done 2893/3489. 0.0892 s / img. ETA=0:01:05
[32m[03/29 17:18:49 d2.evaluation.evaluator]: [0mInference done 2948/3489. 0.0891 s / img. ETA=0:00:59
[32m[03/29 17:18:54 d2.evaluation.evaluator]: [0mInference done 3001/3489. 0.0891 s / img. ETA=0:00:53
[32m[03/29 17:18:59 d2.evaluation.evaluator]: [0mInference done 3053/3489. 0.0890 s / img. ETA=0:00:47
[32m[03/29 17:19:04 d2.evaluation.evaluator]: [0mInference done 3103/3489. 0.0889 s / img. ETA=0:00:42
[32m[03/29 17:19:09 d2.evaluation.evaluator]: [0mInference done 3154/3489. 0.0889 s / img. ETA=0:00:36
[32m[03/29 17:19:14 d2.evaluation.evaluator]: [0mInference done 3206/3489. 0.0888 s / img. ETA=0:00:30
[32m[03/29 17:19:19 d2.evaluation.evaluator]: [0mInference done 3258/3489. 0.0888 s / img. ETA=0:00:25
[32m[03/29 17:19:24 d2.evaluation.evaluator]: [0mInference done 3312/3489. 0.0887 s / img. ETA=0:00:19
[32m[03/29 17:19:29 d2.evaluation.evaluator]: [0mInference done 3368/3489. 0.0887 s / img. ETA=0:00:13
[32m[03/29 17:19:34 d2.evaluation.evaluator]: [0mInference done 3424/3489. 0.0886 s / img. ETA=0:00:07
[32m[03/29 17:19:39 d2.evaluation.evaluator]: [0mInference done 3477/3489. 0.0886 s / img. ETA=0:00:01
[32m[03/29 17:19:41 d2.evaluation.evaluator]: [0mTotal inference time: 0:06:15.671468 (0.107828 s / img per device, on 1 devices)
[32m[03/29 17:19:41 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:05:08 (0.088543 s / img per device, on 1 devices)
[32m[03/29 17:19:42 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[03/29 17:19:42 d2.evaluation.coco_evaluation]: [0mSaving results to ./output/coco_instances_results.json
[32m[03/29 17:19:42 d2.evaluation.coco_evaluation]: [0mEvaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.09s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
COCOeval_opt.evaluate() finished in 1.27 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.33 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.514
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.800
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.577
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.682
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.594
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.629
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.490
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.688
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.842
[32m[03/29 17:19:44 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 51.433 | 79.961 | 53.401 | 37.660 | 57.654 | 68.228 |
[32m[03/29 17:19:44 d2.evaluation.coco_evaluation]: [0mPer-category bbox AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| Cars       | 66.390 | Pedestrian | 36.476 |
Loading and preparing results...
DONE (t=1.06s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
COCOeval_opt.evaluate() finished in 1.89 seconds.
Accumulating evaluation results...
COCOeval_opt.accumulate() finished in 0.37 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.503
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.777
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.510
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.323
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.580
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.579
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.611
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.469
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.672
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.813
[32m[03/29 17:19:50 d2.evaluation.coco_evaluation]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 50.287 | 77.742 | 50.983 | 32.302 | 57.959 | 70.903 |
[32m[03/29 17:19:50 d2.evaluation.coco_evaluation]: [0mPer-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| Cars       | 69.180 | Pedestrian | 31.394 |
[32m[03/29 17:19:50 d2.engine.defaults]: [0mEvaluation results for kittimots_test in csv format:
[32m[03/29 17:19:50 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[03/29 17:19:50 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/29 17:19:50 d2.evaluation.testing]: [0mcopypaste: 51.4331,79.9614,53.4007,37.6605,57.6540,68.2278
[32m[03/29 17:19:50 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[03/29 17:19:50 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[03/29 17:19:50 d2.evaluation.testing]: [0mcopypaste: 50.2870,77.7418,50.9828,32.3016,57.9585,70.9026
evaluated
0015/000152
Time inference:  0.10945558547973633
0016/000027
Time inference:  0.10414623469114304
0019/000115
Time inference:  0.10491905454546213
0019/000222
Time inference:  0.10099608171731234
0019/000324
Time inference:  0.10177322570234537
0019/000570
Time inference:  0.10390616301447153
0019/001035
Time inference:  0.10394545830786228
0020/000630
Time inference:  0.10071026999503374
0020/000688
Time inference:  0.09929031506180763
